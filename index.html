<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Advanced Dental Digitization: Incorporating Shape Priors and
    Multiview Coherence in Diffusion Models for Tooth Model Design">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SVGS: Single-View to 3D Object Editing via Gaussian Splatting</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

</head>

<body>



  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">SVGS: Single-View to 3D Object Editing via Gaussian Splatting</h1>
            <div class="is-size-5 publication-authors">
              <!-- <span class="author-block">
                Yan Tian,</span>
              <span class="author-block">
                Qiutao Song,</span>
              <span class="author-block">
                Karen Egiazarian,</span>
              <span class="author-block">
                Rutkowski,</span>
              <span class="author-block">
                Mahmoud Hassaballah,</span>
              <span class="author-block">
                Teddy Yang,</span>
              <span class="author-block">
                Weiping Ding</span> -->
             
            </div>

            <div class="is-size-5 publication-authors">
              <!-- <span class="author-block">University of Washington,</span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/2011.12948" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->
                <span class="link-block">
                  <a href=""
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Text-driven 3D editing has gained significant attention due to its convenience and user-friendliness. However, traditional 3D editing methods that rely on mesh and point cloud representations often struggle to accurately describe complex objects. In contrast, methods based on implicit 3D representations, such as Neural Radiance Fields (NeRF), can effectively render complex objects but suffer from slow processing speeds and limited control over specific object areas. Additionally, existing methods often face challenges in maintaining consistency and efficiency in multi-view editing. To address these limitations, we propose SVGS, a method that utilizes text instructions to edit objects through 3D Gaussian Splatting (GS). Specifically, given a singleview image and text input, we first employ a 2D editing strategy based on the cross-domain diffusion model to perform editing operations on the input image and generate multi-view consistent edited images. Subsequently, we introduce a GS reconstruction framework based on the edited sparse images to achieve 3D object editing. We compared SVGS with existing baseline methods across various scene settings, and the results demonstrate that SVGS excels in both editing capability and processing speed, marking a significant advancement in 3D editing technology.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Method. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Method</h2>
          <div class="content has-text-justified">
            <img src="./static/images/framework.png" alt="Description of the image">
          </div>
        </div>
      </div>
      <!--/ Method. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Example. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Example</h2>
          <div class="content has-text-justified">
            <img src="./static/images/example.png" alt="Description of the image">
          </div>
        </div>
      </div>
      <!--/ Example. -->
    </div>
  </section>


</body>

</html>
